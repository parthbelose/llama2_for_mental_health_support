{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-dBxkxMeOmIC"
      },
      "outputs": [],
      "source": [
        "#installing and importing all necessary libraries. Make sure you are connected to a T4 GPU on colab or have a GPU with cuda installed\n",
        "!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7 datasets\n",
        "\n",
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    HfArgumentParser,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        "    logging,\n",
        ")\n",
        "from peft import LoraConfig, PeftModel\n",
        "from trl import SFTTrainer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#enter your hugging face token\n",
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "-9iVo-HCWV9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hf_token=input('Enter hugging face token :')"
      ],
      "metadata": {
        "id": "wVcU6tjGXjU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#you can use custom dataset and vary the length of train and eval dataset\n",
        "train_data = load_dataset(\"vibhorag101/phr_mental_therapy_dataset\",split=\"train[:50000]\")\n",
        "eval_data = load_dataset(\"vibhorag101/phr_mental_therapy_dataset\",split=\"train[51000:52000]\")"
      ],
      "metadata": {
        "id": "bpk58cylWeXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Evaluation data length : \",len(eval_data))\n",
        "print(\"Training data length : \",len(train_data))"
      ],
      "metadata": {
        "id": "NBuoo5RTWtXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Parameters\n",
        "model_name = \"meta-llama/Llama-2-7b-chat-hf\" #make sure you have access to this model\n",
        "new_model = \"llama-2-7b-custom\"\n",
        "lora_r = 64\n",
        "lora_alpha = 16\n",
        "lora_dropout = 0.1\n",
        "use_4bit = True\n",
        "bnb_4bit_compute_dtype = \"float16\"\n",
        "bnb_4bit_quant_type = \"nf4\"\n",
        "use_nested_quant = False\n",
        "output_dir = \"./results\"\n",
        "num_train_epochs = 1\n",
        "fp16 = False\n",
        "bf16 = False\n",
        "per_device_train_batch_size = 4\n",
        "per_device_eval_batch_size = 4\n",
        "gradient_accumulation_steps = 1\n",
        "gradient_checkpointing = True\n",
        "max_grad_norm = 0.3\n",
        "learning_rate = 2e-4\n",
        "weight_decay = 0.001\n",
        "optim = \"paged_adamw_32bit\"\n",
        "lr_scheduler_type = \"constant\"\n",
        "max_steps = -1\n",
        "warmup_ratio = 0.03\n",
        "group_by_length = True\n",
        "save_steps = 100\n",
        "logging_steps = 5\n",
        "max_seq_length = None\n",
        "packing = False\n",
        "device_map = {\"\": 0}"
      ],
      "metadata": {
        "id": "t931UDDiXEMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#System prompt\n",
        "system_message=\"You are a helpful and joyous mental therapy assistant. Always answer as helpfully and cheerfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. Do not assume anything. \""
      ],
      "metadata": {
        "id": "kA0A8RcJXOpy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quantization\n",
        "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=use_4bit,\n",
        "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        "    bnb_4bit_use_double_quant=use_nested_quant,\n",
        ")\n",
        "\n",
        "\n",
        "#loading the model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=device_map,\n",
        "    token = hf_token\n",
        ")\n",
        "\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1\n",
        "\n",
        "#loading the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name\n",
        "                                          , trust_remote_code=True)\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=lora_alpha,\n",
        "    lora_dropout=lora_dropout,\n",
        "    r=lora_r,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "# Set training parameters\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    optim=optim,\n",
        "    save_steps=save_steps,\n",
        "    logging_steps=logging_steps,\n",
        "    learning_rate=learning_rate,\n",
        "    weight_decay=weight_decay,\n",
        "    fp16=fp16,\n",
        "    bf16=bf16,\n",
        "    max_grad_norm=max_grad_norm,\n",
        "    max_steps=max_steps,\n",
        "    warmup_ratio=warmup_ratio,\n",
        "    group_by_length=group_by_length,\n",
        "    lr_scheduler_type=lr_scheduler_type,\n",
        "    report_to=\"all\",\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps= 20\n",
        ")\n",
        "# Set supervised fine-tuning parameters\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=train_data,\n",
        "     eval_dataset=eval_data,\n",
        "    peft_config=peft_config,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_arguments,\n",
        "    packing=packing,\n",
        ")\n",
        "trainer.train()\n",
        "trainer.model.save_pretrained(new_model)\n"
      ],
      "metadata": {
        "id": "z4c6xGL4XVRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model\n",
        "logging.set_verbosity(logging.CRITICAL)\n",
        "message=input(\"Enter a command : \")\n",
        "prompt = f\"[INST] <<SYS>>\\n{system_message}\\n<</SYS>>\\n\\{message}[/INST]\"\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=200)\n",
        "result = pipe(prompt)\n",
        "print(result[0]['generated_text'])"
      ],
      "metadata": {
        "id": "4QPSh8Z4X5Jw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# you can add history context by modifying the prompt\n",
        "while True:\n",
        "  message=input(\"Enter a command : \")\n",
        "  prompt = f\"[INST] <<SYS>>\\n{system_message}\\n<</SYS>>\\n\\{message}[/INST]\"\n",
        "  pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=200)\n",
        "  result = pipe(prompt)\n",
        "  print(result[0]['generated_text'])"
      ],
      "metadata": {
        "id": "8ZTT3FJvYSfY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#you can save the model directly but in my case i ran out of gpu memory hence i tried other alternatives\n",
        "model.save_pretrained(model_path)\n",
        "save_pretrained(model_path)"
      ],
      "metadata": {
        "id": "7Si-ivEphb3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pushing the model to hub"
      ],
      "metadata": {
        "id": "W2MUQE9Thsxu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# You can push your model to huggingface\n",
        "# make sure you have a write token\n",
        "hf_repo_id=\"\"# enter your repo name\n",
        "model.push_to_hub(hf_repo_id)\n",
        "tokenizer.push_to_hub(hf_repo_id)"
      ],
      "metadata": {
        "id": "rweAqjekeaXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# then you can directly use your model\n",
        "from transformers import pipeline\n",
        "hf_repo_id=\"\"# enter your repo name\n",
        "pipe = pipeline(\"text-generation\", model=hf_rep_id)\n",
        "message=input(\"Enter a command : \")\n",
        "prompt = f\"[INST] <<SYS>>\\n{system_message}\\n<</SYS>>\\n\\{message}[/INST]\"\n",
        "result = pipe(prompt)\n",
        "print(result[0]['generated_text'])"
      ],
      "metadata": {
        "id": "i2PxLi96fR3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# or load the model\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "hf_repo_id=\"\"# enter your repo name\n",
        "# you can add quantization parameters to your model\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(hf_repo_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(hf_repo_id)\n",
        "\n",
        "\n",
        "pipe= pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=200)\n",
        "message=input(\"Enter a command : \")\n",
        "prompt = f\"[INST] <<SYS>>\\n{system_message}\\n<</SYS>>\\n\\{message}[/INST]\"\n",
        "result = pipe(prompt)\n",
        "print(result[0]['generated_text'])"
      ],
      "metadata": {
        "id": "C_hsO46ufjRc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you don't want to push the mode to hugging face you can try saving the model in drive\n",
        "\n",
        "Merging the adapter model with base model\n",
        "\n",
        "Make sure you restart the session and run the imports and parameters cell"
      ],
      "metadata": {
        "id": "vnyqawqUdirp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge and save the fine-tuned model in google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "model_path = \"/content/drive/MyDrive/llama-2-7b-custom\"  # change to your preferred path\n",
        "# Reload model in FP16 and merge it with LoRA weights\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    low_cpu_mem_usage=True,\n",
        "    return_dict=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=device_map\n",
        ")\n",
        "model = PeftModel.from_pretrained(base_model, new_model)\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "# Reload tokenizer to save it\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "# Save the merged model\n",
        "model.save_pretrained(model_path)\n",
        "tokenizer.save_pretrained(model_path)"
      ],
      "metadata": {
        "id": "Sami8tTGY_Cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loading model from drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM,pipeline\n",
        "model_path=\"\"\n",
        "# you can add quantization parameters to your model\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
        "\n",
        "\n",
        "pipe= pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=200)\n",
        "\n",
        "message=input(\"Enter a command : \")\n",
        "prompt = f\"[INST] <<SYS>>\\n{system_message}\\n<</SYS>>\\n\\{message}[/INST]\"\n",
        "result = pipe(prompt)\n",
        "print(result[0]['generated_text'])"
      ],
      "metadata": {
        "id": "wJAxn25UguxD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}